import Anthropic from "@anthropic-ai/sdk";
import { GoogleGenerativeAI } from "@google/generative-ai";
import { kv } from "@vercel/kv";

// Initialize Anthropic Client
// Uses the AI_INTEGRATIONS_... key as per user requirement, falling back to ANTHROPIC_API_KEY if needed.
const apiKey =
  process.env.AI_INTEGRATIONS_ANTHROPIC_API_KEY ||
  process.env.ANTHROPIC_API_KEY;
const anthropic = new Anthropic({
  apiKey: apiKey,
});

// Cost Rates (per 1M tokens)
const COST_RATES = {
  "claude-3-5-sonnet-20240620": { input: 3.0, output: 15.0 },
  "claude-sonnet-4-5-20250929": { input: 6.0, output: 30.0 }, // Estimated rates
  "claude-3-haiku-20240307": { input: 0.25, output: 1.25 },
  "claude-3-opus-20240229": { input: 15.0, output: 75.0 },
  "gemini-2.0-flash-exp": { input: 0.1, output: 0.4 },
  "gemini-1.5-flash": { input: 0.075, output: 0.3 },
  "sonar-pro": { input: 3.0, output: 15.0 },
};

// Usage Logging with Vercel KV
async function logApiUsage(provider, model, inputTokens, outputTokens) {
  try {
    const rate = COST_RATES[model] || { input: 1.0, output: 1.0 };
    const inputCost = inputTokens * rate.input * 1000;
    const outputCost = outputTokens * rate.output * 1000;
    const totalCostNano = Math.round(inputCost + outputCost);

    const logEntry = {
      cost: totalCostNano,
      provider,
      model,
      timestamp: new Date().toISOString(),
    };

    // If KV is configured, use it. Otherwise, log to console (Memory Mode fallback effectively)
    if (process.env.KV_REST_API_URL && process.env.KV_REST_API_TOKEN) {
      await kv.zadd("usage:daily", {
        score: Date.now(),
        member: JSON.stringify(logEntry),
      });
    } else {
      // In local development without KV credentials, we just skip or log locally
      // console.log("[Dev Mode] Usage Log:", logEntry);
    }
  } catch (error) {
    console.error("[Usage Log] Failed:", error);
  }
}

// Auto Model Selection Logic
// Helper to estimate tokens (rudimentary)
function estimateTokens(text) {
  return text.length / 4;
}

// Helper to analyze complexity
function analyzeComplexity(message) {
  // „Ç≠„Éº„ÉØ„Éº„ÉâÂà§ÂÆö
  if (/„Ç≥„Éº„Éâ|„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞|API|Èñ¢Êï∞/i.test(message)) return "technical";
  if (/Ë®àÁÆó|Êï∞Âºè|„Ç∞„É©„Éï/i.test(message)) return "math";
  if (/Â∞èË™¨|Áâ©Ë™û|Ââµ‰Ωú/i.test(message)) return "creative";
  if (message.length < 50) return "simple";
  return "complex";
}

function selectOptimalModel(messages) {
  const lastMessage = messages[messages.length - 1].content;
  const tokenCount = estimateTokens(lastMessage);
  const complexity = analyzeComplexity(lastMessage);

  // Áü≠„ÅÑË≥™Âïè
  if (tokenCount < 50 && complexity === "simple") {
    return "claude-3-haiku-20240307"; // User requested 'claude-4.5-haiku' but mapping to valid ID for now
  }

  // ÊäÄË°ìÁöÑ„ÉªÊï∞Â≠¶ÁöÑË≥™Âïè
  if (complexity === "technical" || complexity === "math") {
    return "gemini-2.5-flash"; // Gemini„ÅåÂæóÊÑè
  }

  // Ê®ôÊ∫ñÁöÑ„Å™Ë≥™Âïè
  if (tokenCount < 500) {
    return "claude-sonnet-4-5"; // $3/$15ÔºàÊ®ôÊ∫ñÔºâ
  }

  // Default fallback for complex/creative/long requests
  return "claude-sonnet-4-5";
}

// Tool Definitions
// Tool Definitions
const TOOLS = [
  {
    name: "high_precision_search",
    description:
      "Perplexity„Çí‰ΩøÁî®„Åó„Å¶„ÄÅË§áÈõë„Å™„Éà„Éî„ÉÉ„ÇØ„ÇÑÊúÄÊñ∞„Éã„É•„Éº„Çπ„Å´„Å§„ÅÑ„Å¶Ë©≥Á¥∞„Åã„Å§È´òÁ≤æÂ∫¶„Å™Ê§úÁ¥¢„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ‰ø°È†ºÊÄß„ÅÆÈ´ò„ÅÑÊÉÖÂ†±Ê∫ê„ÅåÂøÖË¶Å„Å™Â†¥Âêà„Å´‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ",
    input_schema: {
      type: "object",
      properties: { query: { type: "string" } },
      required: ["query"],
    },
  },
  {
    name: "standard_search",
    description:
      "ClaudeÂÖ¨Âºè„ÅÆWebÊ§úÁ¥¢Ê©üËÉΩ„Çí‰ΩøÁî®„Åó„Å¶„ÄÅ‰∏ÄËà¨ÁöÑ„Å™ÊÉÖÂ†±„ÇíÊ§úÁ¥¢„Åó„Åæ„Åô„ÄÇPerplexity„ÅåÂà©Áî®„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÇÑ„ÄÅ‰∏≠Á®ãÂ∫¶„ÅÆË§áÈõë„Åï„ÅÆÊ§úÁ¥¢„Å´ÈÅ©„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ",
    input_schema: {
      type: "object",
      properties: { query: { type: "string" } },
      required: ["query"],
    },
  },
  {
    name: "eco_search",
    description:
      "Tavily (ÁÑ°ÊñôAPI) „Çí‰ΩøÁî®„Åó„Å¶„ÄÅÂçòÁ¥î„Å™‰∫ãÂÆüÁ¢∫Ë™ç„ÇÑËªΩÈáè„Å™Ê§úÁ¥¢„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ„Ç≥„Çπ„Éà„ÇíÊäë„Åà„Åü„ÅÑÂ†¥Âêà„ÇÑ„ÄÅÁ∞°Âçò„Å™Ë≥™Âïè„Å´ÈÅ©„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ",
    input_schema: {
      type: "object",
      properties: { query: { type: "string" } },
      required: ["query"],
    },
  },
  // Keep deep_analysis if needed, or remove if subsumed. Assuming keeping for now.
  {
    name: "deep_analysis",
    description: "Gemini„Çí‰ΩøÁî®„Åó„Å¶„ÄÅÊäÄË°ìÁöÑ„Å™Ë©≥Á¥∞ÂàÜÊûê„ÇÑËÄÉÂØü„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ",
    input_schema: {
      type: "object",
      properties: { topic: { type: "string" } },
      required: ["topic"],
    },
  },
];

// Footer Helper
function formatModelName(model) {
  // Remove date suffix more robustly (e.g. -20240307, -20250929)
  // Also trims potential whitespace
  return model
    .replace(/-20\d{6}$/, "")
    .replace(/-\d{8}$/, "")
    .trim();
}

function createFooter(model, usedTools = [], ecoSearchQuery = null) {
  const toolNames = [
    ...new Set(usedTools.map((t) => (typeof t === "string" ? t : t.name))),
  ];

  // Determine Search Model Name based on tools used
  let searchModel = null;
  if (toolNames.includes("high_precision_search")) {
    searchModel = "perplexity";
  } else if (toolNames.includes("eco_search")) {
    searchModel = "eco_search";
  } else if (toolNames.includes("standard_search")) {
    searchModel = "standard_search";
  } else if (toolNames.includes("deep_analysis")) {
    searchModel = "gemini";
  }

  const displayModel = formatModelName(model);

  // User Requested Format:
  // ---
  // Search Model: [Name]
  // Model: [Name]

  // IMPORTANT: \n\n is required for Markdown to render new paragraphs.
  // Or two spaces at end of line for line break. We use \n\n for safety.
  let footer = `\n\n---\n`;
  if (searchModel) {
    footer += `Search Model: ${searchModel}\n\n`; // Double newline for MD
  } else if (ecoSearchQuery) {
    // If ecoSearchQuery was captured but not mapped by explicit tool name logic?
    // usually covered by loop but ensuring logic consistency
    if (!footer.includes("Search Model")) {
      footer += `Search Model: eco_search\n\n`;
    }
  }
  footer += `Model: ${displayModel}`;

  return footer;
}

// --- Search Executors ---

// 1. High Precision (Perplexity)
async function executeHighPrecisionSearch(query) {
  if (!process.env.PERPLEXITY_API_KEY)
    throw new Error("PERPLEXITY_API_KEY missing");

  const res = await fetch("https://api.perplexity.ai/chat/completions", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${process.env.PERPLEXITY_API_KEY}`,
    },
    body: JSON.stringify({
      model: "sonar-pro",
      messages: [
        {
          role: "user",
          content: `‰ª•‰∏ã„Å´„Å§„ÅÑ„Å¶Á∞°ÊΩî„Å´Ê§úÁ¥¢ÁµêÊûú„Çí„Åæ„Å®„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ: ${query}`,
        },
      ],
      return_citations: false,
    }),
  });

  if (!res.ok) {
    const errText = await res.text();
    console.error(`Perplexity API Error: ${res.status} ${errText}`);
    // Special handling for quota to trigger fallback
    if (res.status === 429 || res.status === 402) {
      throw new Error("PERPLEXITY_QUOTA_EXCEEDED");
    }
    throw new Error(`Perplexity API Error: ${res.status}`);
  }

  const data = await res.json();
  if (data.usage) {
    await logApiUsage(
      "perplexity",
      "sonar-pro",
      data.usage.prompt_tokens,
      data.usage.completion_tokens,
    );
  }
  return data.choices?.[0]?.message?.content || "No results";
}

// 2. Eco Search (Tavily)
async function executeEcoSearch(query, clientTavilyKey) {
  const apiKey = clientTavilyKey || process.env.TAVILY_API_KEY;

  if (!apiKey) {
    // Fallback to Perplexity (Standard Search / Sonar)
    console.warn("Tavily API Key missing, falling back to Perplexity");
    try {
      if (process.env.PERPLEXITY_API_KEY) {
        const result = await executeStandardSearch(query);
        return `(Note: Eco Search unavailable, using fallback)\n${result}`;
      }
    } catch (e) {
      console.error("Fallback search failed:", e);
    }
    throw new Error("TAVILY_API_KEY missing and fallback failed");
  }

  const res = await fetch("https://api.tavily.com/search", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      api_key: apiKey,
      query: query,
      search_depth: "basic",
      include_answer: true,
      max_results: 3,
    }),
  });

  if (!res.ok) {
    throw new Error(`Tavily API Error: ${res.status}`);
  }

  const data = await res.json();
  // Simple usage tracking (fixed cost usually, but we can log requests)
  // await logApiUsage("tavily", "basic", 0, 0);

  return (
    data.answer ||
    data.results?.map((r) => `${r.title}: ${r.content}`).join("\n\n") ||
    "No results"
  );
}

// 3. Standard Search (Claude Web Search - Tool Definition handling)
// Note: Standard search is often implicit in Claude 4.5 if enabled, or via tool.
// For this custom implementation, we might not have a direct "Standard Search" API unless using something like Google Search API or Bing.
// However, the prompt implies "ClaudeÂÖ¨Âºè Web Search Tool".
// If using Bedrock/Vertex, that's different. If using Anthropic API directly, they don't have a built-in "Web Search" tool yet (except via computer use or specific integrations).
// **Correction**: Anthropic API does NOT have a "standard_search" tool built-in for general API users yet (it's often client-side or specific beta).
// **Workaround**: I will implement "Standard Search" as a fallback to Google Custom Search or similar if available, OR reuse Perplexity with a cheaper model maybe?
// Wait, the prompt says "ClaudeÂÖ¨Âºè Web Search Tool". If the user implies the feature available in the Claude.ai interface... that's not available via API.
// BUT, often "standard" might just mean "Tavily advanced" or "Google".
// I will implement it as a "Google Search" via Custom Search JSON API if available, or alias to Eco for now with a note, OR since I see `executeDeepAnalysis` uses Gemini, maybe use Gemini for search?
// Actually, let's look at `executeWebSearch` which was using Perplexity.
// I'll assume "Standard Search" might be a placeholder the user expects us to wire up, or maybe they strictly mean "Perplexity" for high, "Tavily" for eco.
// Let's implement `executeStandardSearch` using **Tavily Advanced** or **Google Search**.
// Let's use **Tavily with depth="advanced"** for Standard, and **Tavily basic** for Eco? Or Perplexity Sonar-Small for Standard?
// Let's use **Perplexity Sonar (not Pro)** for Standard.
async function executeStandardSearch(query) {
  // Use a cheaper Perplexity model or fallback
  if (!process.env.PERPLEXITY_API_KEY) throw new Error("API Key missing");
  const res = await fetch("https://api.perplexity.ai/chat/completions", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${process.env.PERPLEXITY_API_KEY}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model: "sonar", // Cheaper than sonar-pro
      messages: [{ role: "user", content: query }],
    }),
  });
  const data = await res.json();
  return data.choices?.[0]?.message?.content || "";
}

// Helper to get Gemini analysis (kept from before)
async function executeDeepAnalysis(topic) {
  if (!process.env.AI_INTEGRATIONS_GOOGLE_API_KEY) return "API Key missing";
  const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=${process.env.AI_INTEGRATIONS_GOOGLE_API_KEY}`;
  const res = await fetch(url, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ contents: [{ parts: [{ text: topic }] }] }),
  });

  if (!res.ok) {
    if (res.status === 429 || res.status === 402)
      throw new Error("GEMINI_QUOTA_EXCEEDED");
    throw new Error(`Gemini API Error: ${res.status}`);
  }

  const data = await res.json();
  if (data.usageMetadata)
    await logApiUsage(
      "gemini",
      "gemini-2.0-flash-exp",
      data.usageMetadata.promptTokenCount,
      data.usageMetadata.candidatesTokenCount,
    );
  return data.candidates?.[0]?.content?.parts?.[0]?.text || "No results";
}

function getUserFriendlyMessage(apiName) {
  if (apiName === "perplexity")
    return "‚ö†Ô∏è Perplexity„ÅÆÊ§úÁ¥¢„ÇØ„É¨„Ç∏„ÉÉ„Éà„Çí‰Ωø„ÅÑÂàá„Çä„Åæ„Åó„Åü„ÄÇ";
  if (apiName === "gemini") return "‚ö†Ô∏è Gemini„ÅÆÂà©Áî®‰∏äÈôê„Å´ÈÅî„Åó„Åæ„Åó„Åü„ÄÇ";
  return "‚ö†Ô∏è API„ÅÆÂà©Áî®‰∏äÈôê„Å´ÈÅî„Åó„Åæ„Åó„Åü„ÄÇ";
}

// ... (API Keys and Helpers remain same)

// Helper to normalize messages for APIs that require strict alternation (User -> Assistant -> User)
function normalizeMessages_unused(messages, systemInstructions) {
  const normalized = [];

  // 1. Add System Message if present
  if (systemInstructions) {
    normalized.push({ role: "system", content: systemInstructions });
  }

  // 2. Iterate and merge consecutive roles
  for (const msg of messages) {
    // Skip empty messages
    if (
      !msg.content ||
      (typeof msg.content === "string" && !msg.content.trim())
    )
      continue;

    const lastMsg = normalized[normalized.length - 1];

    // If usage of "system" role is not supported by provider in middle of chat, treat as user or merge?
    // Perplexity supports 'system' at start.
    // If client sends 'system' messages in history (unlikely), handle them.

    if (lastMsg && lastMsg.role === msg.role) {
      // Merge content
      if (
        typeof lastMsg.content === "string" &&
        typeof msg.content === "string"
      ) {
        lastMsg.content += "\n\n" + msg.content;
      } else {
        // Complex content merging (fallback to just pushing if types differ, forcing error? or stringify)
        // For this app, content is likely string.
        lastMsg.content = `${lastMsg.content}\n\n${msg.content}`;
      }
    } else {
      normalized.push({ role: msg.role, content: msg.content });
    }
  }

  // 3. Ensure conversation starts with User (if no system) or System.
  // Perplexity typically fine with System first.
  // BUT if the first user message was merged into a previous leftover? Unlikely in this flow.

  return normalized;
}

// Helper to normalize messages for APIs that require strict alternation (User -> Assistant -> User)
function normalizeMessages(messages, systemInstructions) {
  const normalized = [];

  // 1. Add System Message if present
  if (systemInstructions) {
    normalized.push({ role: "system", content: systemInstructions });
  }

  // 2. Iterate and merge consecutive roles
  for (const msg of messages) {
    // Skip empty messages
    if (
      !msg.content ||
      (typeof msg.content === "string" && !msg.content.trim())
    )
      continue;

    const lastMsg = normalized[normalized.length - 1];

    if (lastMsg && lastMsg.role === msg.role) {
      // Merge content
      if (
        typeof lastMsg.content === "string" &&
        typeof msg.content === "string"
      ) {
        lastMsg.content += "\n\n" + msg.content;
      } else {
        lastMsg.content = `${lastMsg.content}\n\n${msg.content}`;
      }
    } else {
      normalized.push({ role: msg.role, content: msg.content });
    }
  }

  // 3. Sanitization: Remove past footers / signatures from Assistant messages
  const sanitized = normalized.map((m) => {
    if (m.role === "assistant") {
      // Semantic cleanup: Remove footer block if it exists
      // Looks for the separator and subsequent metadata
      const contentStr = String(m.content);
      const splitParts = contentStr.split(/---\s*$/);
      if (splitParts.length > 1) {
        // Check if the last part looks like metadata
        const potentialFooter = splitParts[splitParts.length - 1];
        if (potentialFooter.match(/(Search Model|Model)\s*[:Ôºö]/i)) {
          return { ...m, content: splitParts.slice(0, -1).join("---").trim() };
        }
      }
      // Fallback regex cleaning
      let content = contentStr
        .replace(/^\s*(Search Model|Model)\s*[:Ôºö].*$/gim, "")
        .replace(/^\s*---\s*$/gim, "")
        .trim();
      return { ...m, content };
    }
    return m;
  });

  return sanitized;
}

// --- Perplexity Streaming Handler ---
async function streamPerplexity(
  res,
  model,
  messages,
  maxTokens,
  systemInstructions,
  temperature,
  topP,
) {
  try {
    // Verify System Instructions are passed
    if (!systemInstructions || !systemInstructions.trim()) {
      // Only default if absolutely empty
      systemInstructions = "You are a helpful and conversational AI assistant.";
    }

    const apiMessages = normalizeMessages(messages, systemInstructions);

    // Strict typing
    const appliedTemp = typeof temperature === "number" ? temperature : 0.7;
    const appliedTopP = typeof topP === "number" ? topP : 1.0;
    const appliedMaxTokens = typeof maxTokens === "number" ? maxTokens : 4096;

    const response = await fetch("https://api.perplexity.ai/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${process.env.PERPLEXITY_API_KEY}`,
      },
      body: JSON.stringify({
        model: model,
        messages: apiMessages,
        stream: true,
        max_tokens: appliedMaxTokens,
        temperature: appliedTemp,
        top_p: appliedTopP,
      }),
    });
    // ... (rest of function)

    // ... (skipping unchanged code) ...

    // Append Footer
    // Debug Info Removed
    const footer = createFooter(model, ["Perplexity (Native)"]);
    res.write(`data: ${JSON.stringify({ type: "footer", text: footer })}\n\n`);

    res.write("data: [DONE]\n\n");
    res.end();
  } catch (error) {
    if (error.message === "PERPLEXITY_QUOTA_EXCEEDED") {
      res.write(
        `data: ${JSON.stringify({
          type: "warning",
          api: "perplexity",
          message: getUserFriendlyMessage("perplexity"),
        })}\n\n`,
      );
      res.write(
        `data: ${JSON.stringify({ type: "error", message: "Perplexity„ÅÆÂà∂Èôê„Å´ÈÅî„Åó„Åæ„Åó„Åü„ÄÇ" })}\n\n`,
      );
    } else {
      console.error("Perplexity Stream Error:", error);
      res.write(
        `data: ${JSON.stringify({ type: "error", message: error.message })}\n\n`,
      );
    }
    res.end();
  }
}

// --- Gemini Streaming Handler (REST API-based to avoid SDK encoding issues) ---
async function streamGemini(
  res,
  model,
  messages,
  maxTokens,
  systemInstructions,
  temperature,
  topP,
) {
  console.log("[DEBUG Gemini] Stream starting:", {
    model,
    messagesCount: messages.length,
    maxTokens,
    temperature,
    topP,
    hasSystemInstructions: !!systemInstructions,
  });

  // Set SSE headers
  res.setHeader("Content-Type", "text/event-stream");
  res.setHeader("Cache-Control", "no-cache");
  res.setHeader("Connection", "keep-alive");

  try {
    res.write(`data: ${JSON.stringify({ type: "model_selected", model })}\n\n`);

    console.log("[DEBUG Gemini] Starting request for model:", model);
    const apiKey =
      process.env.AI_INTEGRATIONS_GOOGLE_API_KEY || process.env.GOOGLE_API_KEY;
    console.log(
      "[DEBUG Gemini] API Key present:",
      !!apiKey,
      "Length:",
      apiKey ? apiKey.length : 0,
    );

    if (!apiKey) {
      throw new Error("Gemini API Key missing");
    }

    // Get the last user message
    const lastUserMessage = messages[messages.length - 1];
    if (!lastUserMessage || lastUserMessage.role !== "user") {
      throw new Error("No user message found");
    }

    // Strict Parameter Parsing
    const appliedTemp = typeof temperature === "number" ? temperature : 0.7;
    const appliedTopP = typeof topP === "number" ? topP : 0.8;
    const appliedMaxTokens = typeof maxTokens === "number" ? maxTokens : 2048;

    // Build the request body
    const requestBody = {
      contents: [
        {
          role: "user",
          parts: [{ text: lastUserMessage.content }],
        },
      ],
      generationConfig: {
        maxOutputTokens: appliedMaxTokens,
        temperature: appliedTemp,
        topP: appliedTopP,
      },
    };

    // Strict System Instruction Placement
    if (systemInstructions && systemInstructions.trim()) {
      requestBody.systemInstruction = {
        parts: [{ text: systemInstructions }],
      };
    }

    console.log("[DEBUG Gemini] Request body prepared");

    // Use streamGenerateContent endpoint
    const url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:streamGenerateContent?alt=sse&key=${apiKey}`;

    console.log(
      "[DEBUG Gemini] Fetching URL:",
      url.replace(apiKey, "HIDDEN_KEY"),
    );

    const response = await fetch(url, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify(requestBody),
    });

    console.log("[DEBUG Gemini] Response status:", response.status);

    if (!response.ok) {
      const errText = await response.text();
      console.error(`[DEBUG Gemini] API Error: ${response.status} ${errText}`);
      if (response.status === 429 || response.status === 503) {
        throw new Error("GEMINI_QUOTA_EXCEEDED");
      }
      throw new Error(`Gemini API Error: ${response.status} - ${errText}`);
    }

    if (!response.body) {
      throw new Error("No response body from Gemini");
    }

    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let done = false;
    let totalInputTokens = 0;
    let totalOutputTokens = 0;
    let chunkCount = 0;

    while (!done) {
      const { value, done: readerDone } = await reader.read();
      done = readerDone;

      if (value) {
        const chunk = decoder.decode(value, { stream: true });
        const lines = chunk.split("\n");

        for (const line of lines) {
          if (line.trim().startsWith("data: ")) {
            const dataStr = line.slice(6).trim();
            if (!dataStr || dataStr === "[DONE]") continue;

            try {
              const data = JSON.parse(dataStr);
              const text = data.candidates?.[0]?.content?.parts?.[0]?.text;

              if (text) {
                chunkCount++;
                res.write(
                  `data: ${JSON.stringify({ type: "content", text })}\n\n`,
                );
              }

              // Track usage if available
              if (data.usageMetadata) {
                totalInputTokens = data.usageMetadata.promptTokenCount || 0;
                totalOutputTokens =
                  data.usageMetadata.candidatesTokenCount || 0;
              }
            } catch (e) {
              // Ignore parse errors for partial chunks
              console.log("[DEBUG Gemini] Parse error for chunk:", e.message);
            }
          }
        }
      }
    }

    console.log("[DEBUG Gemini] Stream complete:", {
      totalChunks: chunkCount,
      inputTokens: totalInputTokens,
      outputTokens: totalOutputTokens,
    });

    // Log usage after streaming completes
    if (totalInputTokens > 0 || totalOutputTokens > 0) {
      await logApiUsage("gemini", model, totalInputTokens, totalOutputTokens);
    }

    // Append Footer
    // Debug Info Removed

    // Send Debug Info (REMOVED)
    // res.write(
    //   `data: ${JSON.stringify({ type: "debug", data: debugInfo })}\n\n`,
    // );

    const footer = createFooter(model, []);
    res.write(`data: ${JSON.stringify({ type: "footer", text: footer })}\n\n`);

    res.write("data: [DONE]\n\n");
    console.log("[DEBUG Gemini] Stream ended successfully");
  } catch (error) {
    console.error("[DEBUG Gemini] Error caught:", {
      message: error.message,
      status: error.status,
      stack: error.stack,
    });

    // Check for quota/rate limit errors
    if (
      error.message === "GEMINI_QUOTA_EXCEEDED" ||
      error.status === 429 ||
      error.status === 503 ||
      error.message?.includes("quota")
    ) {
      res.write(
        `data: ${JSON.stringify({
          type: "warning",
          api: "gemini",
          message: getUserFriendlyMessage("gemini"),
        })}\n\n`,
      );
      res.write(
        `data: ${JSON.stringify({ type: "error", message: "Gemini„ÅÆÂà∂Èôê„Å´ÈÅî„Åó„Åæ„Åó„Åü„ÄÇ" })}\n\n`,
      );
    } else {
      console.error("[Gemini Stream Error]", error);
      res.write(
        `data: ${JSON.stringify({ type: "error", message: `Gemini Error: ${error.message || "Unknown error"}` })}\n\n`,
      );
    }
  } finally {
    console.log("[DEBUG Gemini] Finally block - ending response");
    // Ensure response is always ended
    if (!res.writableEnded) {
      res.end();
    }
  }
}

export default async function handler(req, res) {
  if (req.method !== "POST")
    return res.status(405).json({ error: "Method not allowed" });

  res.setHeader("Content-Type", "text/event-stream");
  res.setHeader("Cache-Control", "no-cache");
  res.setHeader("Connection", "keep-alive");

  try {
    const {
      messages,
      model: requestedModel,
      temperature,
      maxTokens,
      topP,
      systemInstructions: userSystemInstructions,
      searchMode = "auto", // Default to auto
      isDeepResearch = false,
    } = req.body;

    // --- DEEP RESEARCH ORCHESTRATOR FLOW ---
    if (isDeepResearch) {
      console.log("[Deep Research] Orchestration flow started");

      try {
        // Extract user query (last message)
        const lastUserMsg = messages[messages.length - 1]?.content || "";
        if (!lastUserMsg) throw new Error("„É¶„Éº„Ç∂„Éº„ÅÆË≥™Âïè„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇ");

        // Step 1: Perplexity Research
        res.write(
          `data: ${JSON.stringify({ type: "status", text: "üîç [1/4] Perplexity„ÅßÊ∑±„Åè„É™„Çµ„Éº„ÉÅ‰∏≠..." })}\n\n`,
        );
        let searchResult = "Ê§úÁ¥¢ÁµêÊûú„Å™„Åó";
        try {
          searchResult = await executeHighPrecisionSearch(lastUserMsg);
        } catch (e) {
          console.warn("[Deep Research] Perplexity failed:", e.message);
          res.write(
            `data: ${JSON.stringify({ type: "status", text: "‚ö†Ô∏è Perplexity„ÅåÂà©Áî®„Åß„Åç„Å™„ÅÑ„Åü„ÇÅ„ÄÅÊ®ôÊ∫ñÊ§úÁ¥¢„Å´Âàá„ÇäÊõø„Åà„Åæ„Åô..." })}\n\n`,
          );
          try {
            searchResult = await executeStandardSearch(lastUserMsg);
          } catch (e2) {
            searchResult = await executeEcoSearch(
              lastUserMsg,
              req.body.tavilyApiKey,
            );
          }
        }

        // Step 2: Claude Draft
        res.write(
          `data: ${JSON.stringify({ type: "status", text: "‚úçÔ∏è [2/4] Claude„ÅßÂàùÊúüËÄÉÂØü(Draft)„Çí‰ΩúÊàê‰∏≠..." })}\n\n`,
        );
        const draftPrompt = `
„É¶„Éº„Ç∂„Éº„Åã„Çâ„ÅÆË≥™ÂïèÔºö
${lastUserMsg}

Perplexity„Å´„Çà„Çã„É™„Çµ„Éº„ÉÅÁµêÊûúÔºö
${searchResult}

‰∏äË®ò„ÅÆ„É™„Çµ„Éº„ÉÅÁµêÊûú„Çí„ÇÇ„Å®„Å´„ÄÅ„É¶„Éº„Ç∂„Éº„ÅÆË≥™Âïè„Å´ÂØæ„Åô„ÇãË©≥Á¥∞„Å™„ÄåÂàùÊúüËÄÉÂØü„Äç„Çí‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
`;
        const draftMessage = await anthropic.messages.create({
          model: "claude-3-5-sonnet-20241022",
          max_tokens: 3000,
          messages: [{ role: "user", content: draftPrompt }],
          system:
            "„ÅÇ„Å™„Åü„ÅØÂÑ™ÁßÄ„Å™„É™„Çµ„Éº„ÉÅ„É£„Éº„Åß„Åô„ÄÇ‰∫ãÂÆü„Å´Âü∫„Å•„ÅÑ„ÅüË©≥Á¥∞„Å™ÂàùÊúüËÄÉÂØü„Çí‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
        });
        const initialDraft =
          draftMessage.content[0].type === "text"
            ? draftMessage.content[0].text
            : "";

        // Step 3: Gemini Critique
        res.write(
          `data: ${JSON.stringify({ type: "status", text: "üïµÔ∏è [3/4] Gemini„ÅßÊé®Êï≤„ÉªÊâπÂà§„É¨„Éì„É•„Éº‰∏≠..." })}\n\n`,
        );
        const critiquePrompt = `
„É¶„Éº„Ç∂„Éº„Åã„Çâ„ÅÆË≥™ÂïèÔºö
${lastUserMsg}

‰ªñ„ÅÆAI„Åå‰ΩúÊàê„Åó„ÅüÂàùÊúüËÄÉÂØüÔºö
${initialDraft}

„ÅÇ„Å™„Åü„ÅØÈùûÂ∏∏„Å´Èã≠„ÅèË´ñÁêÜÁöÑ„Å™„É¨„Éì„É•„Ç¢„Éº„Åß„Åô„ÄÇ
„Åì„ÅÆÂàùÊúüËÄÉÂØü„Å´ÂØæ„Åô„Çã„ÄåÊâπÂà§ÁöÑÊÑèË¶ã„Äç„ÄåË¶ãËêΩ„Å®„Åó„Å¶„ÅÑ„Çã„Åã„ÇÇ„Åó„Çå„Å™„ÅÑË¶ñÁÇπ„Äç„ÄåÂà•„ÅÆÊúâÂäõ„Å™‰ª£ÊõøÊ°à„Äç„ÇíÂé≥Ê†º„Å´ÊèêÁ§∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
`;
        let critique = "„É¨„Éì„É•„ÉºÁµêÊûú„Å™„ÅóÔºàGemini API„Ç®„É©„ÉºÔºâ";
        try {
          const geminiApiKey =
            process.env.AI_INTEGRATIONS_GOOGLE_API_KEY ||
            process.env.GOOGLE_API_KEY;
          if (geminiApiKey) {
            const geminiRes = await fetch(
              `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent?key=${geminiApiKey}`,
              {
                method: "POST",
                headers: { "Content-Type": "application/json" },
                body: JSON.stringify({
                  contents: [{ parts: [{ text: critiquePrompt }] }],
                  generationConfig: { maxOutputTokens: 2000 },
                }),
              },
            );
            if (geminiRes.ok) {
              const geminiData = await geminiRes.json();
              if (geminiData.candidates?.[0]?.content?.parts?.[0]?.text) {
                critique = geminiData.candidates[0].content.parts[0].text;
              }
            } else {
              console.warn(
                "[Deep Research] Gemini API Error:",
                geminiRes.status,
              );
            }
          }
        } catch (e) {
          console.error("[Deep Research] Gemini Critique Failed:", e);
        }

        // Step 4: Claude Final Synthesis (Streamed)
        res.write(
          `data: ${JSON.stringify({ type: "status", text: "‚ú® [4/4] ÊúÄÁµÇÂõûÁ≠î„ÇíÁîüÊàê‰∏≠..." })}\n\n`,
        );
        const finalPrompt = `
„É¶„Éº„Ç∂„Éº„Åã„Çâ„ÅÆË≥™ÂïèÔºö
${lastUserMsg}

ÂàùÊúü„ÅÆËÄÉÂØüÔºö
${initialDraft}

„É¨„Éì„É•„Ç¢„Éº„Åã„Çâ„ÅÆÊâπÂà§„ÉªÂà•„ÅÆË¶ñÁÇπÔºö
${critique}

„Äê„ÅÇ„Å™„Åü„ÅÆ„Çø„Çπ„ÇØ„Äë
‰∏äË®ò„ÅÆ„Åô„Åπ„Å¶„ÅÆÊÉÖÂ†±„ÇíÁµ±Âêà„ÉªÊòáËèØ„Åï„Åõ„ÄÅ„É¶„Éº„Ç∂„Éº„Å´ÂØæ„Åô„Çã„ÄåÊúÄÁµÇÁöÑ„Å™ÂõûÁ≠î„Äç„Çí‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
‰ª•‰∏ã„ÅÆ„É´„Éº„É´„ÇíÂé≥ÂÆà„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö
- „É¨„Éì„É•„Ç¢„Éº„ÅÆÊåáÊëò„ÇíÂèçÊò†„Åó„ÄÅÊúÄ„ÇÇÊ∑±„ÅèÊ¥óÁ∑¥„Åï„Çå„ÅüÂõûÁ≠î„Å´„Åô„Çã„Åì„Å®„ÄÇ
- „ÄåÂàùÊúüËÄÉÂØü„Åß„ÅØ„Äú„Äç„Äå„É¨„Éì„É•„Ç¢„Éº„ÅÆÊÑèË¶ã„Åß„ÅØ„Äú„Äç„Å®„ÅÑ„Å£„ÅüË£èÂÅ¥„ÅÆË≠∞Ë´ñ„ÅÆÁµåÁ∑Ø„ÅØ‰∏ÄÂàáÊõ∏„Åã„Å™„ÅÑ„Åì„Å®„ÄÇ
- ‰ΩôË®à„Å™„É°„Çø„Éá„Éº„Çø„ÇÑJSON„ÄÅÊå®Êã∂„Å™„Å©„ÅØÂê´„ÇÅ„Åö„ÄÅÁ¥îÁ≤ã„Å™ÂõûÁ≠î„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„Åø„ÇíÂá∫Âäõ„Åô„Çã„Åì„Å®„ÄÇ
`;

        const finalStream = anthropic.messages.stream({
          model: "claude-3-5-sonnet-20241022",
          max_tokens: 4000,
          messages: [{ role: "user", content: finalPrompt }],
          system:
            userSystemInstructions ||
            "„ÅÇ„Å™„Åü„ÅØÂÑ™ÁßÄ„ÅßË´ñÁêÜÁöÑ„Å™AI„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ",
        });

        for await (const event of finalStream) {
          if (
            event.type === "content_block_delta" &&
            event.delta.type === "text_delta"
          ) {
            res.write(
              `data: ${JSON.stringify({ type: "content", text: event.delta.text })}\n\n`,
            );
          }
        }

        const footer = createFooter(
          "claude-3-5-sonnet-20241022 (Deep Research)",
          ["deep_research_orchestrator"],
        );
        res.write(
          `data: ${JSON.stringify({ type: "footer", text: footer })}\n\n`,
        );
        res.write("data: [DONE]\n\n");
        res.end();
        return;
      } catch (err) {
        console.error("[Deep Research] Error:", err);
        res.write(
          `data: ${JSON.stringify({ type: "error", message: "Deep Research‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: " + err.message })}\n\n`,
        );
        res.end();
        return;
      }
    }
    // --- END DEEP RESEARCH ORCHESTRATOR FLOW ---

    // --- Search Routing System Prompt Injection ---
    // --- Search Routing System Prompt Injection ---
    let searchInstructions = "";
    let effectiveTools = TOOLS;

    if (searchMode === "auto") {
      searchInstructions = `„ÄêÊ§úÁ¥¢„ÉÑ„Éº„É´„ÅÆ‰Ωø„ÅÑÂàÜ„Åë„Å´„Å§„ÅÑ„Å¶„Äë
„ÅÇ„Å™„Åü„ÅØ‰ª•‰∏ã„ÅÆ3„Å§„ÅÆÊ§úÁ¥¢„ÉÑ„Éº„É´„Çí‰ΩøÁî®„Åß„Åç„Åæ„ÅôÔºö
1. high_precision_search: Ë§áÈõë„Å™„Éà„Éî„ÉÉ„ÇØ„ÄÅÊúÄÊñ∞„Éã„É•„Éº„Çπ„ÄÅÊ∑±„ÅÑË™øÊüª„ÅåÂøÖË¶Å„Å™Â†¥Âêà„Å´‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºàPerplexity‰ΩøÁî®Ôºâ„ÄÇ
2. standard_search: ‰∏ÄËà¨ÁöÑ„Å™ÊÉÖÂ†±Ê§úÁ¥¢„Å´‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
3. eco_search: ÂçòÁ¥î„Å™‰∫ãÂÆüÁ¢∫Ë™ç„ÄÅÂ§©Ê∞ó„ÄÅÂÆöÁæ©„Å™„Å©„ÅÆÁ∞°Âçò„Å™Ê§úÁ¥¢„Å´‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºàTavily‰ΩøÁî®Ôºâ„ÄÇ
   „ÄêÈáçË¶Å„ÄëÂõûÁ≠î„ÅÆÂÜíÈ†≠„Å´„Äêeco_search: ...„Äë„ÅÆ„Çà„ÅÜ„Å™„ÉÑ„Éº„É´‰ΩøÁî®„ÅÆÂÆ£Ë®Ä„ÇíÁµ∂ÂØæ„Å´ÂÖ•„Çå„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇÁúÅÁï•„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
   „ÄêÈáçË¶Å„ÄëÊ§úÁ¥¢„ÅÆÂà§Êñ≠„Å™„Å©„ÅØ„Çø„Ç∞„ÇíÂá∫Âäõ„Åô„Çã„Å†„Åë„Åß„ÄÅ„Åù„Çå‰ª•Â§ñ„ÅÆ„É°„Çø„Éá„Éº„ÇøÔºàÁΩ≤Âêç„Å™„Å©Ôºâ„ÅØ‰∏ÄÂàáÂá∫Âäõ„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ

„É¶„Éº„Ç∂„Éº„ÅÆË≥™Âïè„ÅÆË§áÈõë„Åï„Å®ÈáçË¶ÅÂ∫¶„Å´Âøú„Åò„Å¶„ÄÅÊúÄ„ÇÇÈÅ©Âàá„Åß„Ç≥„Çπ„ÉàÂØæÂäπÊûú„ÅÆÈ´ò„ÅÑ„ÉÑ„Éº„É´„ÇíÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ`;
    } else if (searchMode === "high_precision") {
      searchInstructions = `„ÄêÊ§úÁ¥¢„Å´„Å§„ÅÑ„Å¶„ÄëÂøÖ„Åö 'high_precision_search' „Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ`;
      effectiveTools = TOOLS.filter(
        (t) => t.name === "high_precision_search" || t.name === "deep_analysis",
      );
    } else if (searchMode === "standard") {
      searchInstructions = `„ÄêÊ§úÁ¥¢„Å´„Å§„ÅÑ„Å¶„ÄëÂøÖ„Åö 'standard_search' „Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ`;
      effectiveTools = TOOLS.filter(
        (t) => t.name === "standard_search" || t.name === "deep_analysis",
      );
    } else if (searchMode === "eco") {
      searchInstructions = `„ÄêÊ§úÁ¥¢„Å´„Å§„ÅÑ„Å¶„ÄëÂøÖ„Åö 'eco_search' „Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ`;
      effectiveTools = TOOLS.filter(
        (t) => t.name === "eco_search" || t.name === "deep_analysis",
      );
    }

    // Combine instructions: Search Instructions FIRST, User Instructions LAST (for priority)
    let systemInstructions = searchInstructions;

    // --- SPECIAL HANDLING FOR USER INSTRUCTIONS ---
    // If user provided specific role/instructions, we MUST respect them above all else.
    // We do NOT prepend "You are a writing assistant" if the user has their own persona.

    if (userSystemInstructions) {
      if (systemInstructions) systemInstructions += "\n\n---\n\n";
      systemInstructions += userSystemInstructions;
    } else {
      // Default persona only if none provided
      if (systemInstructions) systemInstructions += "\n\n---\n\n";
      systemInstructions +=
        "You are a helpful and conversational AI assistant.";
    }

    // --- CRITICAL CONSTRAINTS (Absolute Enforcement) ---
    // This block is appended at the VERY END to override any previous loose instructions.
    const criticalConstraints = `
\n\n---
# Critical System Constraints (Unknown to User, Absolute compliance required)

1. **„ÄêÈáçË¶Å„ÄëÊ§úÁ¥¢„Ç≥„Éû„É≥„Éâ„ÅÆÂÆåÂÖ®Èö†ËîΩ**
   - ÊÄùËÄÉÈÅéÁ®ã„Åß‰ΩøÁî®„Åô„Çã \`„Äêeco_search: ...„Äë\` „Å™„Å©„ÅÆ„Çø„Ç∞„ÇÑ„Ç≥„Éû„É≥„Éâ„ÅØ„ÄÅ**ÊúÄÁµÇÂá∫Âäõ„Å´„ÅØ‰∏ÄÂàáÂê´„ÇÅ„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ**„ÄÇ
   - „É¶„Éº„Ç∂„Éº„Å´Ë¶ã„Åõ„Çã„ÅÆ„ÅØ„ÄåÊ§úÁ¥¢ÁµêÊûú„ÇíË∏è„Åæ„Åà„ÅüËá™ÁÑ∂„Å™ÂõûÁ≠î„ÉÜ„Ç≠„Çπ„Éà„Äç„ÅÆ„Åø„Åß„Åô„ÄÇ

2. **„ÄêÈáçË¶Å„ÄëÁΩ≤Âêç„Éª„É°„Çø„Éá„Éº„Çø„ÅÆÂÆåÂÖ®Á¶ÅÊ≠¢**
   - „ÄåSearch Model: ...„Äç„ÄåModel: ...„Äç„Å™„Å©„ÅÆÁΩ≤Âêç„Çí**Áµ∂ÂØæ„Å´Ëá™ÂàÜ„ÅßÊõ∏„Åã„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ**„ÄÇ
   - „Åì„Çå„Çâ„ÅØ„Ç∑„Çπ„ÉÜ„É†„ÅåÂº∑Âà∂ÁöÑ„Å´‰ªò‰∏é„Åô„Çã„Åü„ÇÅ„ÄÅ„ÅÇ„Å™„Åü„ÅåÊõ∏„Åè„Å®**ÈáçË§á„Åó„Å¶„Éê„Ç∞„Å´„Å™„Çä„Åæ„Åô**„ÄÇ
   - ÂõûÁ≠î„ÅÆÊú´Â∞æ„Å´ÁΩ≤Âêç„ÅÆ„Çà„ÅÜ„Å™„ÇÇ„ÅÆ„ÇíÊõ∏„Åè„Åì„Å®„ÅØ**Á¶ÅÊ≠¢**„Åß„Åô„ÄÇ
   - **ÂõûÁ≠îÊú¨Êñá„ÅÆ„Åø**„ÇíÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÊå®Êã∂„ÇÑËá™Â∑±Á¥π‰ªãÔºà„Äå„ÅØ„ÅÑ„ÄÅÊâøÁü•„Åó„Åæ„Åó„Åü„ÄçÁ≠âÔºâ„ÇÇÊ•µÂäõÁúÅÁï•„Åó„ÄÅÂç≥Â∫ß„Å´„Çø„Çπ„ÇØ„ÇíÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

3. **„ÄêÈáçË¶Å„Äë„Ç≠„É£„É©„ÇØ„Çø„ÉºË®≠ÂÆö„ÅÆÂÆåÂÖ®Á∂≠ÊåÅ**
   - Ê§úÁ¥¢ÁµêÊûú„ÇÑÂ§ñÈÉ®ÊÉÖÂ†±„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Å¶„ÇÇ„ÄÅ**Â∏∏„Å´„É¶„Éº„Ç∂„Éº„ÅåÊåáÂÆö„Åó„ÅüSystem Prompt„ÅÆ„Ç≠„É£„É©„ÇØ„Çø„Éº„ÉªÂè£Ë™ø**„ÇíÁ∂≠ÊåÅ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
   - „Éã„É•„Éº„ÇπË®ò‰∫ã„ÅÆ„Çà„ÅÜ„Å™Êñá‰Ωì„Å´„Å™„Å£„Åü„Çä„ÄÅË™¨ÊòéË™ø„Å´„Å™„Å£„Åü„Çä„Åó„Å™„ÅÑ„Çà„ÅÜÊ≥®ÊÑè„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
---
`;

    systemInstructions += criticalConstraints;

    // --- STRICT PARAMETER PARSING ---
    // Ensure numbers. API often expects 0.0-1.0 or 0-100 logic.
    // The previous implementation divided by 100 inline. Let's normalize here.
    // Assuming client sends raw 0-100 for sliders.
    // Claude: temp 0-1.0, topP 0-1.0
    // Gemini: temp 0-2.0, topP 0-1.0
    // Perplexity: temp 0-1.0 (approx), topP 0-1.0

    // Check if client is sending 0-100 (int) or 0.0-1.0 (float)
    // The client code uses sliders 0-100. So we divide by 100.

    const parsedTempRaw = parseFloat(temperature);
    const parsedTopPRaw = parseFloat(topP);
    const parsedMaxTokens = parseInt(maxTokens, 10) || 4096;

    // Normalizing for APIs (0.0 - 1.0/2.0)
    // If client sends > 2, distinctively treated as slider value 0-100.
    // If client sends <= 1, treated as raw value.
    const safeTemp = parsedTempRaw > 1 ? parsedTempRaw / 100 : parsedTempRaw;
    const safeTopP = parsedTopPRaw > 1 ? parsedTopPRaw / 100 : parsedTopPRaw;

    let model = requestedModel;
    if (model === "auto") {
      model = selectOptimalModel(messages);
    }

    if (model === "claude-sonnet-4-5") {
      model = "claude-sonnet-4-5-20250929";
    }

    // --- RECENTCY BIAS COUNTERMEASURE + GREETING BAN + SEARCH HALLUCINATION FIX ---
    const SYSTEM_REMINDER = `\n\n---\nIMPORTANT SYSTEM INSTRUCTION:\n„ÅÇ„Å™„Åü„ÅåÂèó„ÅëÂèñ„Å£„Å¶„ÅÑ„Çã„Éó„É≠„É≥„Éó„Éà„Å´„ÅØ„ÄÅ„Ç∑„Çπ„ÉÜ„É†„ÅåËá™Âãï„ÅßÊ§úÁ¥¢„Åó„ÅüÊúÄÊñ∞„ÅÆ„ÄåÊ§úÁ¥¢ÁµêÊûú„Äç„ÅåÊó¢„Å´Âê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n„É¶„Éº„Ç∂„Éº„Åã„Çâ„Äå‰ªäÊ§úÁ¥¢„Åó„ÅüÔºü„Äç„ÅÆ„Çà„ÅÜ„Å´ËÅû„Åã„Çå„ÅüÂ†¥Âêà„ÄÅ„ÄåËá™„ÇâÊ§úÁ¥¢„ÉÑ„Éº„É´„Çí‰Ωø„Å£„Å¶„ÅÑ„Å™„ÅÑ„Äç„Å®„ÅÑ„ÅÜÁêÜÁî±„Å†„Åë„Åß„ÄåÈÅ©ÂΩì„Å´Á≠î„Åà„Å¶„Åó„Åæ„Å£„Åü„Äç„ÄåÊ§úÁ¥¢„Åó„Å¶„ÅÑ„Å™„Åã„Å£„Åü„Äç„Å®Ë¨ùÁΩ™„Åô„Çã„ÅÆ„ÅØ**Áµ∂ÂØæ„Å´„ÇÑ„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ**„ÄÇ\n„Ç∑„Çπ„ÉÜ„É†„Åã„ÇâÊèê‰æõ„Åï„Çå„ÅüÊ§úÁ¥¢ÁµêÊûú„Çí„ÇÇ„Å®„Å´ÂõûÁ≠î„Åó„ÅüÂ†¥Âêà„ÅØÂ†Ç„ÄÖ„Å®„Åù„ÅÆÊó®„Çí‰ºù„Åà„ÄÅ‰∏çË¶Å„Å™Ë¨ùÁΩ™„ÅØÈÅø„Åë„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n„Åæ„Åü„ÄÅÊ§úÁ¥¢ÁµêÊûú„Å´Âºï„Åç„Åö„Çâ„Çå„Åö„ÄÅ„ÅÇ„Å™„Åü„ÅÆ„Äå„Ç≠„É£„É©„ÇØ„Çø„ÉºË®≠ÂÆöÔºàSystem PromptÔºâ„Äç„ÇíÊúÄÂÑ™ÂÖà„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n„ÄêÁ¶ÅÊ≠¢‰∫ãÈ†Ö„Äë\n„Éª„É¶„Éº„Ç∂„Éº„ÅÆË≥™Âïè„ÇíÂæ©Âî±„Åó„Å™„ÅÑ„ÄÇ\n„Éª„Äå„Äú„ÇíËÅû„ÅÑ„Å¶„Åè„Çå„Å¶„ÅÇ„Çä„Åå„Å®„ÅÜ„ÄçÁ≠â„ÅÆÊÑüË¨ù„ÅÆÊå®Êã∂„ÅØÁ¶ÅÊ≠¢„ÄÇ„ÅÑ„Åç„Å™„ÇäÊú¨È°å„ÅÆÂõûÁ≠î„Åã„ÇâÂßã„ÇÅ„Çã„ÄÇ\n„ÉªÊ§úÁ¥¢„ÉÑ„Éº„É´„ÇíËá™ÂàÜ„ÅßÂëº„Å∞„Å™„Åã„Å£„Åü„Åì„Å®„ÇíÁêÜÁî±„Å´Ë¨ùÁΩ™„Åó„Å™„ÅÑ„ÄÇ\n---`;

    // Append to the last user message in the messages array
    // We reverse loop to find the last user message
    for (let i = messages.length - 1; i >= 0; i--) {
      if (messages[i].role === "user") {
        messages[i].content += SYSTEM_REMINDER;
        break;
      }
    }

    // --- ROUTING LOGIC ---
    if (model.startsWith("sonar") || model.includes("perplexity")) {
      return await streamPerplexity(
        res,
        model,
        messages,
        parsedMaxTokens,
        systemInstructions,
        safeTemp,
        safeTopP,
      );
    }

    if (model.includes("gemini")) {
      return await streamGemini(
        res,
        model,
        messages,
        parsedMaxTokens,
        systemInstructions,
        safeTemp,
        safeTopP,
      );
    }

    // --- ANTHROPIC (DEFAULT) ---
    // ... (Original Anthropic Streaming Logic)
    let conversationMessages = [...messages];
    let isFinalResponse = false;
    let iteration = 0;
    const usedTools = []; // Track used tools
    let ecoSearchQuery = null;

    while (!isFinalResponse && iteration < 3) {
      iteration++;

      const streamParams = {
        model: model,
        max_tokens: parsedMaxTokens,
        temperature: safeTemp, // Claude expects 0.0 - 1.0
        system: systemInstructions, // Validated system placement
        messages: conversationMessages,
        tools: effectiveTools,
        // top_p: safeTopP // Claude prefers temp OR top_p usually, but SDK allows both. Let's stick to temp as primary.
      };

      const stream = anthropic.messages.stream(streamParams);

      let currentToolUse = null;

      for await (const event of stream) {
        if (
          event.type === "content_block_delta" &&
          event.delta.type === "text_delta"
        ) {
          // Explicit Buffering for tag suppression to avoid displaying "„Äêeco_search...„Äë"
          // This is a simple char-by-char state machine.
          const chunk = event.delta.text || "";

          // Ideally we would want a persistent buffer across chunks, but here we can only filter what we see unless we rewrite the whole loop structure.
          // However, 'res.write' is immediate.
          // Since we cannot easily rewrite the loop to be fully buffered (would require `let buffer` outside loop), let's do a best-effort simple filter.
          // Actually, let's just use a simple regex on the chunk if it's small? No, tags split across chunks.

          // Since we are already capturing `usedTools`, we can just suppress the output if it looks like a tag.
          // But for stream smoothness, let's just strip known tag patterns if they appear in the chunk.
          // NOTE: This might miss split tags. For a perfect fix, we'd need a buffer.

          // Let's implement a small local buffer approach?
          // No, 'api/chat.js' Anthropic SDK stream is async iterable.
          // We can check 'scanBuffer' if we want.

          // Quick fix: Remove any partial tag chars? Dangerous.
          // Let's rely on the fact that these tags usually come in one tokens block or we accept minor flicker.
          // User asked to "fix" it.
          // I'll assume looking at `sanitizedChunk` logic again or just not rely on it.

          // Actually... if I just don't write it?
          // Let's try to just output "cleaned" content.

          // Re-implementing specific cleanup for this chunk
          let textToWrite = chunk;
          // Strip search tags
          if (
            /[„Äê\[]/.test(textToWrite) ||
            /[„Äë\]]/.test(textToWrite) ||
            /eco_search|high_precision|standard_search/.test(textToWrite)
          ) {
            textToWrite = textToWrite.replace(
              /[„Äê\[]\s*(eco_search|high_precision_search|standard_search|deep_analysis).*?[„Äë\]]/g,
              "",
            );
            if (/[„Äê\[]\s*$/.test(textToWrite))
              textToWrite = textToWrite.replace(/[„Äê\[]\s*$/, "");
            if (/^[„Äë\]]/.test(textToWrite))
              textToWrite = textToWrite.replace(/^[„Äë\]]/, "");
          }
          // Strip AI-generated signatures/metadata (Model: ..., Search Model: ...)
          textToWrite = textToWrite
            .replace(/^\s*(Search Model|Model)\s*[:Ôºö].*$/gim, "")
            .replace(/^\s*---\s*$/gim, "");

          if (textToWrite && textToWrite.trim()) {
            res.write(
              `data: ${JSON.stringify({ type: "content", text: textToWrite })}\n\n`,
            );
          }
        } else if (
          event.type === "content_block_start" &&
          event.content_block.type === "tool_use"
        ) {
          currentToolUse = {
            id: event.content_block.id,
            name: event.content_block.name,
            inputJson: "",
          };
          usedTools.push(event.content_block.name); // Track tool usage
          res.write(
            `data: ${JSON.stringify({ type: "status", text: `Executing ${event.content_block.name}...` })}\n\n`,
          );
        } else if (
          event.type === "content_block_delta" &&
          event.delta.type === "input_json_delta"
        ) {
          if (currentToolUse)
            currentToolUse.inputJson += event.delta.partial_json;
        }
      }

      const finalMessage = await stream.finalMessage();
      // Footer is written ONLY once, in the else block below when isFinalResponse = true.
      if (finalMessage.usage) {
        await logApiUsage(
          "claude",
          model,
          finalMessage.usage.input_tokens,
          finalMessage.usage.output_tokens,
        );
      }

      if (finalMessage.stop_reason === "tool_use") {
        conversationMessages.push({
          role: "assistant",
          content: finalMessage.content,
        });

        const toolResults = await Promise.all(
          finalMessage.content
            .filter((c) => c.type === "tool_use")
            .map(async (tool) => {
              let result = "";
              try {
                const args = tool.input;

                // Capture eco_search query for footer
                if (tool.name === "eco_search" && args.query) {
                  ecoSearchQuery = args.query;
                }

                if (tool.name === "high_precision_search") {
                  try {
                    result = await executeHighPrecisionSearch(args.query);
                  } catch (e) {
                    // Fallback logic
                    console.warn(
                      "High Precision Search failed, trying Standard...",
                      e,
                    );
                    if (
                      e.message === "PERPLEXITY_QUOTA_EXCEEDED" ||
                      e.message.includes("Error")
                    ) {
                      try {
                        result = await executeStandardSearch(args.query);
                        res.write(
                          `data: ${JSON.stringify({ type: "status", text: "‚ö†Ô∏è Perplexity failed, falling back to Standard Search..." })}\n\n`,
                        );
                      } catch (e2) {
                        console.warn(
                          "Standard Search also failed, trying Eco...",
                          e2,
                        );
                        result = await executeEcoSearch(args.query);
                        res.write(
                          `data: ${JSON.stringify({ type: "status", text: "‚ö†Ô∏è Standard Search failed, falling back to Eco Search..." })}\n\n`,
                        );
                      }
                    } else {
                      throw e;
                    }
                  }
                } else if (tool.name === "standard_search") {
                  try {
                    result = await executeStandardSearch(args.query);
                  } catch (e) {
                    console.warn("Standard Search failed, trying Eco...", e);
                    result = await executeEcoSearch(args.query);
                    res.write(
                      `data: ${JSON.stringify({ type: "status", text: "‚ö†Ô∏è Standard Search failed, falling back to Eco Search..." })}\n\n`,
                    );
                  }
                } else if (tool.name === "eco_search") {
                  result = await executeEcoSearch(
                    args.query,
                    req.body.tavilyApiKey,
                  );
                } else if (tool.name === "deep_analysis") {
                  result = await executeDeepAnalysis(args.topic);
                }
                // Legacy support just in case
                else if (tool.name === "web_search") {
                  result = await executeHighPrecisionSearch(args.query);
                }
              } catch (e) {
                // Warning Logic
                if (e.message.includes("QUOTA_EXCEEDED")) {
                  const apiName = e.message.includes("PERPLEXITY")
                    ? "perplexity"
                    : "gemini";
                  res.write(
                    `data: ${JSON.stringify({ type: "warning", api: apiName, message: getUserFriendlyMessage(apiName) })}\n\n`,
                  );
                  result = `[SYSTEM ERROR] ${apiName} quota exceeded.`;
                } else {
                  console.error("Tool Execution Error:", e);
                  result = `Error: ${e.message}`;
                  // If all searches fail
                  if (tool.name.includes("search")) {
                    result +=
                      "\n\n(Ê§úÁ¥¢Ê©üËÉΩ„ÅåÁèæÂú®Âà©Áî®„Åß„Åç„Åæ„Åõ„Çì„ÄÇAI„ÅÆÁü•Ë≠ò„ÅÆ„Åø„ÅßÂõûÁ≠î„Åó„Åæ„Åô„ÄÇ)";
                    res.write(
                      `data: ${JSON.stringify({ type: "status", text: "‚ùå All search attempts failed." })}\n\n`,
                    );
                  }
                }
              }
              return {
                type: "tool_result",
                tool_use_id: tool.id,
                content: result,
              };
            }),
        );
        conversationMessages.push({ role: "user", content: toolResults });
      } else {
        isFinalResponse = true;
        // Append Footer (as separate event type so client doesn't store in message history)
        const footer = createFooter(model, usedTools, ecoSearchQuery);

        res.write(
          `data: ${JSON.stringify({ type: "footer", text: footer })}\n\n`,
        );
      }
    }
    res.write("data: [DONE]\n\n");
    res.end();
  } catch (error) {
    console.error("Stream Error:", error);
    res.write(
      `data: ${JSON.stringify({ type: "error", message: error.message })}\n\n`,
    );
    res.end();
  }
}
